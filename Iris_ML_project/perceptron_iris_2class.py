"""Iris Perceptron

Automatically generated by Colaboratory.

https://simonepeirone.it/posts/ml-01-perceptron/

additional:
https://www.bogotobogo.com/python/scikit-learn/Perceptron_Model_with_Iris_DataSet.php

"""

import numpy as np # processing
import pandas as pd #easy data import

import matplotlib.pyplot as plt # plotting
from mpl_toolkits import mplot3d #plotting

from sklearn.model_selection import train_test_split #preparation of data (how many for training/testing)
from sklearn.metrics import accuracy_score # calculate easy the accuracy
from sklearn.datasets import load_iris # get iris data set

# implementation of our perceptron
class Perceptron:
    """
    Perceptron neuron
    """

    def __init__(self, learning_rate=0.1):
        """
        instantiate a new Perceptron

        :param learning_rate: coefficient used to tune the model
        response to training data
        """
        self.learning_rate = learning_rate
        self._b = 0.0  # y-intercept
        self._w = None  # weights assigned to input features
        # count of errors during each iteration
        self.misclassified_samples = []

    def fit(self, x: np.array, y: np.array, n_iter=10):
        """
        fit the Perceptron model on the training data 
        (here oure percpetrons are learning)

        :param x: samples to fit the model on
        :param y: labels of the training samples
        :param n_iter: number of training iterations 
        """
        self._b = 0.0
        self._w = np.zeros(x.shape[1])
        self.misclassified_samples = []

        for _ in range(n_iter):
            # counter of the errors during this training iteration
            errors = 0
            for xi, yi in zip(x, y):
                # for each sample compute the update value
                update = self.learning_rate * (yi - self.predict(xi))
                # and apply it to the y-intercept and weights array
                self._b += update
                self._w += update * xi
                errors += int(update != 0.0)

            self.misclassified_samples.append(errors)

    def f(self, x: np.array) -> float:
        """
        compute the output of the neuron 
        (we can test the knowledge of our perceptrons)
        :param x: input features
        :return: the output of the neuron
        """
        return np.dot(x, self._w) + self._b

    def predict(self, x: np.array):
        """
        convert the output of the neuron to a binary output
        :param x: input features
        :return: 1 if the output for the sample is positive (or zero),
        -1 otherwise
        """
        return np.where(self.f(x) >= 0, 1, -1)


# now lets start with our classification task on iris by loading the data
"""
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
# download and convert the csv into a DataFrame


from sklearn.datasets import load_iris
iris = load_iris() 

iris = datasets.load_iris()
#iris =  pd.read_csv("Iris.csv")


iris_df = pd.DataFrame(iris['data'], columns=iris['feature_names'])
iris_df['species'] = iris['target']


#df =  pd.read_csv('Iris.csv')
#df = pd.read_csv(url, header=None)
#df.head()
"""

data = load_iris()
df = pd.DataFrame(data.data, columns=data.feature_names)
df.head()

# extract the label column
y = df.iloc[:, 4].values
# extract features (taking only 3 features)
x = df.iloc[:, 0:3].values

fig = plt.figure(1)
ax = plt.axes(projection='3d')

ax.set_title('Iris data set')
ax.set_xlabel("Sepal length in width (cm)")
ax.set_ylabel("Sepal width in width (cm)")
ax.set_zlabel("Petal length in width (cm)")

# plot the samples
ax.scatter(x[:50, 0], x[:50, 1], x[:50, 2], color='red',
           marker='o', s=4, edgecolor='red', label="Iris Setosa")
ax.scatter(x[50:100, 0], x[50:100, 1], x[50:100, 2], color='blue',
           marker='^', s=4, edgecolor='blue', label="Iris Versicolour")
ax.scatter(x[100:150, 0], x[100:150, 1], x[100:150, 2], color='green',
           marker='x', s=4, edgecolor='green', label="Iris Virginica")

plt.legend(loc='upper left')

# reduce the dimensionality of the data (get only first two classes)
x = x[0:100, 0:2]  
y = y[0:100]

# plot Iris Setosa samples
plt.figure(2)
plt.scatter(x[:50, 0], x[:50, 1], color='red', marker='o', label='Setosa')
# plot Iris Versicolour samples
plt.scatter(x[50:100, 0], x[50:100, 1], color='blue', marker='x',
            label='Versicolour')

# show the legend
plt.xlabel("Sepal length")
plt.ylabel("Petal length")
plt.legend(loc='upper left')


# Now we start with classification :-)

# map the labels to a binary integer value
y = np.where(y == 'Iris-setosa', 1, -1)

# standardization of the input features
plt.figure(3)
plt.hist(x[:, 0], bins=100)
plt.title("Features before standardization")
plt.savefig("./before.png", dpi=300)


x[:, 0] = (x[:, 0] - x[:, 0].mean()) / x[:, 0].std()
x[:, 1] = (x[:, 1] - x[:, 1].mean()) / x[:, 1].std()

plt.figure(4)
plt.hist(x[:, 0], bins=100)
plt.title("Features after standardization")
plt.savefig("./after.png", dpi=300)

# split the data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25,
                                                    random_state=0)

# train the model
iterations=10;
classifier = Perceptron(learning_rate=0.01)
classifier.fit(x_train, y_train, n_iter=iterations)

# lets have a look to the test data output and compare to the given label
y_class_out = classifier.f(x_test)
y_class_pred = classifier.predict(x_test)
print('class_out / label')
for idx in range(len(y_class_out)):
    print(round(y_class_out[idx], 3),y_test[idx])

accuracy = accuracy_score(classifier.predict(x_test), y_test)
print("Iterations %d: accuracy %f" % (iterations, accuracy))

# plot the number of errors during each iteration
plt.figure(5)
plt.plot(range(1, len(classifier.misclassified_samples) + 1),
         classifier.misclassified_samples, marker='o')
plt.xlabel('Epoch')
plt.ylabel('Errors')



from matplotlib.colors import ListedColormap

def plot_decision_regions(x, y):
    resolution = 0.001
    
    # define a set of markers
    markers = ('o', 'x')
    # define available colors
    cmap = ListedColormap(('red', 'blue'))
    
    # select a range of x containing the scaled test set
    x1_min, x1_max = x[:, 0].min() - 0.5, x[:, 0].max() + 0.5
    x2_min, x2_max = x[:, 1].min() - 0.5, x[:, 1].max() + 0.5
    
    # create a grid of values to test the classifier on
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    
    # plot the decision region...
    plt.figure()
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # ...and the points from the test set
    for idx, c1 in enumerate(np.unique(y)):
        plt.scatter(x=x[y == c1, 0],
                    y=x[y == c1, 1], 
                    alpha=0.8, 
                    c=cmap(idx), 
                    marker=markers[idx], 
                    label=c1)
    

plot_decision_regions(x_test, y_test)
plt.show()

